<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.78.2" />


<link rel="shortcut icon" href="https://raw.githubusercontent.com/EthanHolleman/ethanholleman.github.io/main/images/favicon.ico?token=AK5PLQBSVED75NNE3LLASBK74LOGS" />


<title>Data and the Vietnam War - Ethan Holleman</title>


<meta name="author" content="Ethan Holleman" />


<meta name="description" content="A minimal Hugo theme with nice theme color." />


<meta name="keywords" content="blogs, data wrangling" />

<meta property="og:title" content="Data and the Vietnam War" />
<meta property="og:description" content="Turning the jungle into punch cards During the early optimistic days of the summer 2020 quarantine I watched Ken Burn&rsquo;s fantastic 10 part 18-hour series on the Vietnam war. It is by far the most accessible and compressive body of work on the subject. Burn&rsquo;s starts you off pre-WWI so you really get a comprehensive picture of things.
One of the aspects of the war that fascinated me the most was the push by the the then Secretary of Defense, Robert McNamara, to quantify as much of the war as was computationally possible. One of the most storage intensive of McNamara&rsquo;s efforts was the Hamlet Evaluation System, which attempted to quantify the degree to which ~12,000 small, rural Vietnamese villages had been pacified." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ethanholleman.github.io/posts/vietnam_data/" />
<meta property="og:image" content="https://ethanholleman.github.io/img/og.png"/>
<meta property="article:published_time" content="2020-12-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-12-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://ethanholleman.github.io/img/og.png"/>

<meta name="twitter:title" content="Data and the Vietnam War"/>
<meta name="twitter:description" content="Turning the jungle into punch cards During the early optimistic days of the summer 2020 quarantine I watched Ken Burn&rsquo;s fantastic 10 part 18-hour series on the Vietnam war. It is by far the most accessible and compressive body of work on the subject. Burn&rsquo;s starts you off pre-WWI so you really get a comprehensive picture of things.
One of the aspects of the war that fascinated me the most was the push by the the then Secretary of Defense, Robert McNamara, to quantify as much of the war as was computationally possible. One of the most storage intensive of McNamara&rsquo;s efforts was the Hamlet Evaluation System, which attempted to quantify the degree to which ~12,000 small, rural Vietnamese villages had been pacified."/>





<link rel="stylesheet" href="https://ethanholleman.github.io/assets/css/fuji.min.css" />





<script async src="https://www.googletagmanager.com/gtag/js?id=G-84NGXGRHE8"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-84NGXGRHE8');
</script>


</head>

<body data-theme="auto">
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://ethanholleman.github.io/">Ethan Holleman</a>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://ethanholleman.github.io/posts/vietnam_data/">Data and the Vietnam War</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2020-12-12</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/blogs">blogs</a>&nbsp;<a href="/tags/data-wrangling">data wrangling</a>&nbsp;</span>

    </div>
    
    
    <div class="post-content markdown-body">
        <h2 id="turning-the-jungle-into-punch-cards">Turning the jungle into punch cards</h2>
<p>During the early optimistic days of the summer 2020 quarantine I watched
Ken Burn&rsquo;s fantastic 10 part <em>18-hour</em> series on the Vietnam war. It is by far
the most accessible and compressive body of work on the subject. Burn&rsquo;s starts you
off <em>pre-WWI</em> so you really get a comprehensive picture of things.</p>
<p>One of the aspects of the war that fascinated me the most was the push by the
the then Secretary of Defense, Robert McNamara, to quantify as much of the war
as was computationally possible. One of the most storage intensive of McNamara&rsquo;s
efforts was the Hamlet Evaluation System, which attempted to quantify the degree
to which ~12,000 small, rural Vietnamese villages had been pacified. A RAND
corporation report found the program was producing 90,000 pages of data every
month. More than could have ever been useful.</p>
<figure>
    <img src="/posts/images/doc.png"/> <figcaption>
            <h4>Example of data produced by the Hamlet Evaluation Program showing change in hamlet classifications over time</h4>
        </figcaption>
</figure>

<p>This was the data produced by just one wartime metrics program. Even
if the DoD had the raw 60s era computing power and the army of FORTRAN programers
that would have been needed to wrangle it all, the metrics themselves were questionable
at best. One of the historians interviewed as apart of Burn&rsquo;s series said something
along the lines of &ldquo;When you can&rsquo;t measure whats counts, you make what can count
the measure&rdquo;.</p>
<p>I was interested in actually seeing what that data looked like in its raw form.
What where programmers of the era actually looking at and wrangling when some
Army big-wig said &ldquo;We need to be producing 90,000 pages of data a month&rdquo;? So I
did some googling and dug into a couple documents hosted my the National Archives
to try and get a picture of what Vietnam looked like from the perspective of a
punch card.</p>
<h2 id="the-phung-hoang-mangement-information-system">The Phung Hoang Mangement Information System</h2>
<p>The degree of documentation relating to the Hamlet Evaluation System that
survives to today is, somewhat unsurprisingly, extremely large. So picking one
out to dive into was a less than analytical process.</p>
<p>A came across the <a href="https://catalog.archives.gov/id/17364134" target="_blank">Phung Hoang Mangement Information System</a>,
PHMIS, (MACV Document Number DAR R33 CM-01A, March 1972) which was later replaced with
the National Police Infrastructure Analysis Subsystem; a database cataloging
the personal information of Vietnamese who where suspected of or convicted of
aiding communist forces as part of the Hamlet Evaluation Program. This entry was
interesting to me because it had both the technical documentation needed to
actually make sense of the data and because of its historical context. The
PHMIS was used as a catalogue for the operations of the
<a href="https://en.wikipedia.org/wiki/Phoenix_Program" target="_blank">Phoenix program</a>
a CIA headed counter-insurgency operation that among other techniques, employed
torture and assassination to identify and kill Viet-Cong and Viet-Cong
collaborators.</p>
<figure>
    <img src="/posts/images/flowchart.png"/> <figcaption>
            <h4>Flowchart depicting data schema of the Phung Hoang Mangement Information System, March 1972 report</h4>
        </figcaption>
</figure>

<p>The Phoenix Program was, deservingly, a locus of controversy within a storm of
controversies surrounding US operations in Vietnam, eventually building to a
series of congressional hearings in 1971. The data stored in this National Archive
entry, in some way, is the bureaucratic reflection of all the individual stories
and lives impacted by this corner of red-scare induced mania.</p>
<h2 id="getting-into-the-data">Getting into the data</h2>
<p>The PHMIS National Archive entry contains one main data file and some aggregated
technical documentation.</p>
<ul>
<li><code>RG330.PHMIS.P6972</code>: The actual binary data file. Refered to as <code>RG330</code></li>
<li><code>222.1SPA.pdf</code>: Historical background on the PHMIS</li>
<li><code>222.1DP.pdf</code>: Technical documentation on format of <code>RG330</code></li>
</ul>
<p>At first I was a bit lost as to where to start after thumbing through the technical
documentation. At first I naively tried to read <code>RG330</code> as UTF-8 but then
soon remembered unicode was not created until the 1980s.</p>
<p>Thankfully I found the technical specifications summary provided by the National
Archives which had this critical information.</p>
<p><img class="img-zoomable" src="/posts/images/tech_summ.png" alt="" />
</p>
<p>The number of records, the length of each record and critically the encoding:
EBCDIC. At this point I had no idea what EBCDIC encoding was but some quick
googling corrected that. Basically, EBCDIC (Extended Binary Coded Decimal Interchange Code)
was created by IBM in the late 1950s and used a perplexing (see EBCDIC wikipedia page
criticism and humor) eight-bit character encoding for mainframe computers.</p>
<figure>
    <img src="https://upload.wikimedia.org/wikipedia/en/thumb/1/16/Blue-punch-card-front-horiz_top-char-contrast-stretched.png/1920px-Blue-punch-card-front-horiz_top-char-contrast-stretched.png"/> <figcaption>
            <h4>Example of EBCDIC encoded punch card. </h4>
        </figcaption>
</figure>

<p>I almost stopped here, but the Python community came through once again and thanks to
<a href="https://pypi.org/user/roskakori/" target="_blank">Thomas Aglassinger</a> you can successfully run
the command</p>
<pre><code>pip install ebcdic
</code></pre>
<p>and have a number of EBCDIC encodings to work with right in Python. And the
secrets of <code>RG330</code> can be revealed in their UTF-8 glory with the Python
snippet below.</p>
<pre><code class="language-python">import ebcdic
filepath = './RG330.PHMIS.P6972'
data = open(filepath, 'rb').read().decode('cp1141')
</code></pre>
<h2 id="wrangling">Wrangling</h2>
<p>The next step was to get the decoded mass of information into something
that could be output to a csv file and would be much easier to work with. The
technical documentation that let us know <code>RG330</code> was EDCDIC encoded also says
that the record length is 319 so my approach was to go with that and pray to
the data-wrangling Gods.</p>
<pre><code class="language-python">step = 319
print(data[:step])
</code></pre>
<p>gives</p>
<pre><code>01000005A207000069120H691270122E70125041100002CB  34KKKKK5C12070103001BM########################00000ä    00000ä                           00000ä    00000ä             00000ä    00000ä               00000ä    00000ä               0ä               ########################################################################
</code></pre>
<p>which as a first attempt does not actually look that bad. The National Archives
documentation notes that a personally identifying information in the database
has been redacted for public use - which is showing up as the <code>#</code> characters.
A memo from December 10, 1992 states the following.</p>
<p><img class="img-zoomable" src="/posts/images/memo.png" alt="" />
</p>
<p>I used this to verify the correctness of my naive parsing approach. If everything lines
up, the only characters we should be finding in columns 73-96, 248-271, 272-295
and 296-319 are <code>#</code>s.</p>
<pre><code class="language-python">redacted_base_1 = [(73, 96), (248, 271), (272, 295), (296, 319)]
redacted_base_0 = [(r[0] - 1, r[1]) for r in redacted_base_1]

for redacted_region in redacted_base_0:
    start, end = redacted_region
    assert set(data[start:end]).pop() == &quot;#&quot;
</code></pre>
<p>Does not produce any assertion errors so lets expand to the whole dataset.</p>
<pre><code class="language-python">for i in range(0, len(data), 319):
    record = data[i:i+step]
    for redacted_region in redacted_base_0:
        start, end = redacted_region
        assert set(record[start:end]).pop() == &quot;#&quot;
</code></pre>
<p>Which also does not produce any assertion errors, so things are looking pretty
good at this point. The two items of main concern are the <code>ä</code> characters and the
large amount of whitespace. Some more investigation revealed the reason for the
whitespace. The document was originally stored in a variable record length format
and was converted to a fixed length by the National Archives at some point. Relevant
documentation in its original photocopy glory below.</p>
<p><img class="img-zoomable" src="/posts/images/blanks.png" alt="" />
</p>
<p>While I am not as certain about the cause of the diacritic a character, I believe
it is due to the use of <a href="https://en.wikipedia.org/wiki/Binary-coded_decimal#Zoned_decimal" target="_blank">zoned decimal formatting</a> in some of the fields. The
documentation notes its use in general, but does not provide specific indices.
Since it is not affecting the integrity of the actual record parsing I am
ignoring apparent zoned decimal fields for the time being.</p>
<p>The technical documentation also defines the range of each field within a
record that we can use to make the final output file more explicitly
delimitated. A sample of which is shown below.</p>
<p><img class="img-zoomable" src="/posts/images/fields.png" alt="" />
</p>
<p>So now it is just a matter of creating a function that will cut up a record
based on the fields defined in this table. I first used <code>PyPDF2</code> to pull out
the text from the table I was interested in.</p>
<pre><code class="language-python">import PyPDF2
pdf = open('222.1DP.pdf', 'rb')  
reader = PyPDF2.PdfFileReader(pdf)  
pages = ''.join([reader.getPage(p).extractText() for p in [20, 21, 22, 23]])
print(pages[:100])
</code></pre>
<p>Which looks like this</p>
<pre><code>INPUT, OUTPUT, MASTER DEFINITION (Excluding Reports) I. PAGE 1 OF 4 5. DATE PREPARED 9/1/77 2. NAME 
</code></pre>
<p>Its a jumble but thankfully, there are some patterns that can be exploited to reduce manual work
required to get everything correctly formatted. Specifically after the range of
each field it is followed by either an <code>A</code> or a <code>N</code>, signifying if that data is
numeric or alphanumeric. We can use this pattern in a <a href="https://en.wikipedia.org/wiki/Regular_expression" target="_blank">regex</a> to roughly pull out
what we need.</p>
<pre><code class="language-python">import re
finder = re.compile(r'(.{1,7}) (\d+-?\d*) (N|A)')
matches = finder.findall(pages)
for m in matches[:10]:
    print(m[0], m[1])
</code></pre>
<p>Which prints</p>
<pre><code>. PROVe 1-2
SE'O'fO 3-8
6 ATLRG 9
 !XX)RP 10
DPROV 11-12
. DDIsr 13-14
2 DVllL 15-16
. IDATX 23-26
 BDA'IX 33-36
POORP 37
</code></pre>
<p>While this is by no means perfect it is looking a lot better. From here I just
cleaned things up manually, eventually creating a text file that looks like this</p>
<pre><code>PROVC 1 2
SEQNO 3 8
ATLRG 9
DOORP 10

.
.
.

ALTVCI 239 247
MOM 248 271
POP 272 295
ALIAS 296 319
</code></pre>
<p>Now we can finally create the function that will create the csv file we are
after!</p>
<p>First I created a dictionary that could be used to slice each &ldquo;row&rdquo; of the
raw data.</p>
<pre><code class="language-python">import csv
table = &quot;parse_table.txt&quot;
spacing_dict = {
    r[0]: [int(i) for i in r[1:]] for r in csv.reader(open(table), delimiter=' ')
    }

# adjust for base 1 to base 0 indexing
for field in spacing_dict:
    spacing_dict[field][0] -= 1

# append next index for single index fields for easier slicing in next step
for field_name in spacing_dict:
    if len(spacing_dict[field_name]) == 1:
        val = spacing_dict[field_name][0]
        spacing_dict[field_name].append(val+1)
</code></pre>
<p>Then I created a function that would actually do the slicing on each raw data
row and return a dictionary with the field names as keys and the data in each
field&rsquo;s respective domain as values.</p>
<pre><code class="language-python">def raw_data_to_field_dict(raw_data, spacing_dict):
    field_dict = {}
    for field_name in spacing_dict:
        start, end = spacing_dict[field_name]
        field_dict[field_name] = raw_data[start:end]
    return field_dict
</code></pre>
<p>Now we can test this out to see if we can write our csv file.</p>
<pre><code class="language-python">def write_csv_from_raw_data(raw_data, spacing_dict, outname='data.csv'):
    csv_rows = []
    for i in range(0, len(data), step):
        row = data[i:i+step]
        csv_rows.append(raw_data_to_field_dict(row, spacing_dict))
    
    with open(outname, 'w') as handle:
        writer = csv.DictWriter(handle, fieldnames=spacing_dict.keys())
        writer.writeheader()
        writer.writerows(csv_rows)
</code></pre>
<p>The function executes without incident and writes a csv file that looks like
the sample below.</p>
<pre><code>PROVC	SEQNO	ATLRG	DOORP	DPROV	DDIST	DVllL
1	5			7	0	0
1	11			1	2	0
1	12			1	2	0
</code></pre>
<p>Much easier to read!</p>
<h1 id="visualizing">Visualizing</h1>
<p>Now that the data is in a more accessible format we can start to take examine
what it looks like using R and the <code>ggplot2</code> package.</p>
<h2 id="arrests-by-year">Arrests by year</h2>
<p><img class="img-zoomable" src="/posts/images/arrests_by_year.png" alt="" />
</p>
<p>Unsurprisingly most arrests took place between 70 and 72. Although there were
some extreme early outliers in 1900 and 1903 which are likely data entry errors.</p>
<h2 id="individual-statuses">Individual statuses</h2>
<p>Fields 126-131 contain the <code>OPINFO</code> information, which the technical documentation
describes as a group containing the following information, again as described by
the technical documentation.</p>
<ul>
<li><code>STATUS</code>: The status of an individual.</li>
<li><code>TARGET</code>: The type of target, general of specific.</li>
<li><code>LEVEL</code>: The level of operation. Corps, Division, etc.</li>
<li><code>FORCE</code>: Type of action force which was responsible for the neutralization.</li>
<li><code>DEFAC</code>: Place of detention.</li>
</ul>
<p>This is a lot of interesting information. These values are all stored as one
letter codes and the documentation provides tables for translating them, like
the one below which is used to translate the <code>STATUS</code> code.</p>
<p><img class="img-zoomable" src="/posts/images/table1.png" alt="" />
</p>
<p>First we can look at just statuses of all individuals in the database to get
as sense for what was happening to the people targeted by Project Phoenix.</p>
<p><img class="img-zoomable" src="/posts/images/ind_status.png" alt="" />
</p>
<p>While the <code>STATUS</code> field is missing for about a third of individuals in the
database, clearly the most common outcomes were &ldquo;captured&rdquo;, &ldquo;killed&rdquo; or &ldquo;rallied&rdquo;. While
&ldquo;captured&rdquo; and &ldquo;killed&rdquo; are relatively unambiguous, there is not further explanation
of what &ldquo;rallied&rdquo; refers to that I could find.</p>
<p>This same data can also be broken down in a few interesting ways. We can plot
the same <code>STATUS</code> data but split into into subplots based on the <code>FORCE</code> that
was responsible for the (coldly bureaucratically termed) neutralization.</p>
<p><img class="img-zoomable" src="/posts/images/status_by_force.png" alt="" />
\</p>
<p>Using this plot, we can see that &ldquo;Regional Forces&rdquo; where the most active group,
with &ldquo;ARVN Main Forces&rdquo;, &ldquo;Popular Forces&rdquo;, &ldquo;Provincial Reconnaissance Unit&rdquo;
making up much of the remainder.</p>
<p>This plot also shows that US Forces, at least according to this database, where
not as nearly as directly involved as organizations that can be grouped into
the &ldquo;South Vietnamese Allies&rdquo; category.</p>
<p>Lastly, I was interested in what this program looked like over time. Individuals
that were captured (opposed to killed outright) usually had a value in their
<code>SADATX</code> field: &ldquo;Date of sentence action in YYMM order&rdquo;. I used this as a proxy
for a given group&rsquo;s activity over time, granted this would be easily skewed in
the case one group was tasked explicitly with capturing while another was tasked with killing.
Plotting <code>SADATX</code> vs the number of individuals for all groups listed by the <code>FORCES</code> field
produced the plot below.</p>
<p><img class="img-zoomable" src="/posts/images/captures_by_date_by_force.png" alt="" />
</p>
<h1 id="there-is-still-much-to-be-said">There is still much to be said</h1>
<p>I only looked at a small part of this dataset, but there is still much more to
be gleaned. If you would like to play around with the data yourself you can
download the csv file I produced <a href="/posts/data/PHMIS_data.csv">from this link</a>.
You can also download all the documentation I referenced from the
National Archives entry <a href="https://catalog.archives.gov/id/17364134" target="_blank">at this link</a>.</p>
<p>Thank you for reading.</p>
<p>-eth</p>

    </div>
</article>




            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/projects/">Projects</a>
            </li>
            
            <li>
                <a href="/posts/">Posts</a>
            </li>
            
            <li>
                <a href="/cv/holleman_cv.pdf">Download CV</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/ethanholleman" target="_blank"><span>GitHub</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/blogs/">blogs</a>
            </span>
            
            <span>
                <a href="/tags/christmas-card/">christmas card</a>
            </span>
            
            <span>
                <a href="/tags/crystallography/">crystallography</a>
            </span>
            
            <span>
                <a href="/tags/data-wrangling/">data wrangling</a>
            </span>
            
            <span>
                <a href="/tags/day-trips/">day trips</a>
            </span>
            
            <span>
                <a href="/tags/fish/">fish</a>
            </span>
            
            <span>
                <a href="/tags/fishing/">fishing</a>
            </span>
            
            <span>
                <a href="/tags/guides/">guides</a>
            </span>
            
            <span>
                <a href="/tags/poker/">poker</a>
            </span>
            
            <span>
                <a href="/tags/programming/">programming</a>
            </span>
            
            <span>
                <a href="/tags/python/">Python</a>
            </span>
            
            <span>
                <a href="/tags/r/">R</a>
            </span>
            
            <span>
                <a href="/tags/research/">research</a>
            </span>
            
        </div>
    </div>
    
    
    
    
</aside>
        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/projects/">Projects</a>
            </li>
            
            <li>
                <a href="/posts/">Posts</a>
            </li>
            
            <li>
                <a href="/cv/holleman_cv.pdf">Download CV</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/ethanholleman" target="_blank"><span>GitHub</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/blogs/">blogs</a>
            </span>
            
            <span>
                <a href="/tags/christmas-card/">christmas card</a>
            </span>
            
            <span>
                <a href="/tags/crystallography/">crystallography</a>
            </span>
            
            <span>
                <a href="/tags/data-wrangling/">data wrangling</a>
            </span>
            
            <span>
                <a href="/tags/day-trips/">day trips</a>
            </span>
            
            <span>
                <a href="/tags/fish/">fish</a>
            </span>
            
            <span>
                <a href="/tags/fishing/">fishing</a>
            </span>
            
            <span>
                <a href="/tags/guides/">guides</a>
            </span>
            
            <span>
                <a href="/tags/poker/">poker</a>
            </span>
            
            <span>
                <a href="/tags/programming/">programming</a>
            </span>
            
            <span>
                <a href="/tags/python/">Python</a>
            </span>
            
            <span>
                <a href="/tags/r/">R</a>
            </span>
            
            <span>
                <a href="/tags/research/">research</a>
            </span>
            
        </div>
    </div>
    
    
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2020-2020
                <a href="https://ethanholleman.github.io/">Ethan Holleman</a>
                 | <a href="https://github.com/EthanHolleman/ethanholleman.github.io">Source code</a> 
                
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/combine/npm/medium-zoom@1.0.6,npm/lazysizes@5.2.2"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.21.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.21.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>


</body>

</html>