[{"content":"The Covid-19 Genomics UK Consortium has been collecting and sequencing thousands of COVID-19 genomes from patients in the UK and around the world.\nAll of their data is publicly available. Here I played around with the phylogenetic tree they have created from global alignments of all the genomes they have sequenced.\nYou can download the tree in Newick format from their data page which also hosts sequences and the alignment files.\nVisualizing the COVID-19 phylogenetic tree by country of origin Genome count by country Note this plot is log scale in the y-axis.\n16 most prevalent UK COVID-19 lineages Density plots showing the number of genomes of the 16 most prevalent lineages detected by COG-UK.\nCov-lineages has a lot of good information on all of these lineages, including B.1.177 which has recently set off alarms as the new \u0026ldquo;mutant UK strain\u0026rdquo;.\nCode used to make the above plots can be viewed here.\n","date":"2020-12-21","permalink":"https://ethanholleman.github.io/posts/cog_sars/","tags":["programming","R","blogs"],"title":"Plotting COG-UK Data"},{"content":"Apartment Tour Taken right after move in.\nSkys during the wildfires Those are not clouds.\nDavis campus cows Around Davis Hiking in Winters, CA\nFishing at Putah Creek\nThanksgiving Bubbles Lots cat pictures have been taken\nCalifornia changes a man ","date":"2020-12-20","permalink":"https://ethanholleman.github.io/posts/christmas_card/","tags":["blogs","christmas card"],"title":"2020 Christmas Card Bonus Pictures"},{"content":"Running stack sizes Shows each players cumulative stack over all nights played.\n Winnings by date Plot of each players winnings (stack at the end of the night - buy in) over time.\n All games have been played virtually, for info on how we host these see this post\n","date":"2020-12-18","permalink":"https://ethanholleman.github.io/posts/running_winnings/","tags":["poker","blogs"],"title":"Poker Nights Running Results"},{"content":"Recently a few of my friends and I wanted to do virtual Poker night. We where not interested in playing for cash and so I started looking around for an online platform to make it happen.\nPretty much everything that comes up with a cursory google search did not satisfy my basic requirements but eventually I came across PlayingCards.io which provided what I think is the best solution for free, causal quarantine poker nights.\nStep 1 is to set up your \u0026ldquo;table\u0026rdquo; with PlayingCards. Use this link to start building a table. Later once you finish setting everything up you can share the link to the table and others will be able to join, see and manipulate the objects on the table.\nYou can use the automation buttons to handle things like dealing, the flop, turn, river shuffling, as well as betting. The room I set up looks like this.\nYou can deal cards to each player using the Deal button and show each round of community cards using the respectively labeled button. Empty sets the pot to 0, Reset sets the pot to 0 and all player stacks to 100 and Reveal shows all players cards.\nThe only thing that is a little clunky is betting and collecting your winnings. The automations allow you to add or subtract a value from a counter (which are used to keep track of each player\u0026rsquo;s stack and the current pot) but not read from the state of another counter. This means you cannot create a button that adds the current value in the pot to a player\u0026rsquo;s stack - this needs to be done by hand.\nAlso anyone can press any button at any time; just something to keep in mind if playing with anyone that you thought of when reading that sentence.\nNote that cards placed into the \u0026ldquo;hand\u0026rdquo; area can only be seen by the player that put them there. This makes keeping your pocket cards secret from everyone else possible.\nIf you don\u0026rsquo;t want to set up your own room from scratch you can download the room I set up and then load from that file.\nNo Poker is actually done over Zoom, it is only used as the video call medium.\nIt definitely look a couple hands to get used to using the interface but once things got going it worked really well. We found it also helps to keep track of your stack either on paper or using small text boxes on the virtual table.\nHere is a gif of me playing out a hand. Normally the other players would take their cards out of the dealt slots and keep them in their \u0026ldquo;hand\u0026rdquo; area where they are not visible to other players until showdown. If a player decides to fold they place their cards back into their dealt slots.\n","date":"2020-12-15","permalink":"https://ethanholleman.github.io/posts/zoom_poker/","tags":["poker","blogs"],"title":"Casual virtual Poker with Zoom and PlayingCards.io"},{"content":"Turning the jungle into punch cards During the early optimistic days of the summer 2020 quarantine I watched Ken Burn\u0026rsquo;s fantastic 10 part 18-hour series on the Vietnam war. It is by far the most accessible and compressive body of work on the subject. Burn\u0026rsquo;s starts you off pre-WWI so you really get a comprehensive picture of things.\nOne of the aspects of the war that fascinated me the most was the push by the the then Secretary of Defense, Robert McNamara, to quantify as much of the war as was computationally possible. One of the most storage intensive of McNamara\u0026rsquo;s efforts was the Hamlet Evaluation System, which attempted to quantify the degree to which ~12,000 small, rural Vietnamese villages had been pacified. A RAND corporation report found the program was producing 90,000 pages of data every month. More than could have ever been useful.\n  Example of data produced by the Hamlet Evaluation Program showing change in hamlet classifications over time   This was the data produced by just one wartime metrics program. Even if the DoD had the raw 60s era computing power and the army of FORTRAN programers that would have been needed to wrangle it all, the metrics themselves were questionable at best. One of the historians interviewed as apart of Burn\u0026rsquo;s series said something along the lines of \u0026ldquo;When you can\u0026rsquo;t measure whats counts, you make what can count the measure\u0026rdquo;.\nI was interested in actually seeing what that data looked like in its raw form. What where programmers of the era actually looking at and wrangling when some Army big-wig said \u0026ldquo;We need to be producing 90,000 pages of data a month\u0026rdquo;? So I did some googling and dug into a couple documents hosted my the National Archives to try and get a picture of what Vietnam looked like from the perspective of a punch card.\nThe Phung Hoang Mangement Information System The degree of documentation relating to the Hamlet Evaluation System that survives to today is, somewhat unsurprisingly, extremely large. So picking one out to dive into was a less than analytical process.\nA came across the Phung Hoang Mangement Information System, PHMIS, (MACV Document Number DAR R33 CM-01A, March 1972) which was later replaced with the National Police Infrastructure Analysis Subsystem; a database cataloging the personal information of Vietnamese who where suspected of or convicted of aiding communist forces as part of the Hamlet Evaluation Program. This entry was interesting to me because it had both the technical documentation needed to actually make sense of the data and because of its historical context. The PHMIS was used as a catalogue for the operations of the Phoenix program a CIA headed counter-insurgency operation that among other techniques, employed torture and assassination to identify and kill Viet-Cong and Viet-Cong collaborators.\n  Flowchart depicting data schema of the Phung Hoang Mangement Information System, March 1972 report   The Phoenix Program was, deservingly, a locus of controversy within a storm of controversies surrounding US operations in Vietnam, eventually building to a series of congressional hearings in 1971. The data stored in this National Archive entry, in some way, is the bureaucratic reflection of all the individual stories and lives impacted by this corner of red-scare induced mania.\nGetting into the data The PHMIS National Archive entry contains one main data file and some aggregated technical documentation.\n RG330.PHMIS.P6972: The actual binary data file. Refered to as RG330 222.1SPA.pdf: Historical background on the PHMIS 222.1DP.pdf: Technical documentation on format of RG330  At first I was a bit lost as to where to start after thumbing through the technical documentation. At first I naively tried to read RG330 as UTF-8 but then soon remembered unicode was not created until the 1980s.\nThankfully I found the technical specifications summary provided by the National Archives which had this critical information.\nThe number of records, the length of each record and critically the encoding: EBCDIC. At this point I had no idea what EBCDIC encoding was but some quick googling corrected that. Basically, EBCDIC (Extended Binary Coded Decimal Interchange Code) was created by IBM in the late 1950s and used a perplexing (see EBCDIC wikipedia page criticism and humor) eight-bit character encoding for mainframe computers.\n  Example of EBCDIC encoded punch card.    I almost stopped here, but the Python community came through once again and thanks to Thomas Aglassinger you can successfully run the command\npip install ebcdic  and have a number of EBCDIC encodings to work with right in Python. And the secrets of RG330 can be revealed in their UTF-8 glory with the Python snippet below.\nimport ebcdic filepath = './RG330.PHMIS.P6972' data = open(filepath, 'rb').read().decode('cp1141')  Wrangling The next step was to get the decoded mass of information into something that could be output to a csv file and would be much easier to work with. The technical documentation that let us know RG330 was EDCDIC encoded also says that the record length is 319 so my approach was to go with that and pray to the data-wrangling Gods.\nstep = 319 print(data[:step])  gives\n01000005A207000069120H691270122E70125041100002CB 34KKKKK5C12070103001BM########################00000ä 00000ä 00000ä 00000ä 00000ä 00000ä 00000ä 00000ä 0ä ########################################################################  which as a first attempt does not actually look that bad. The National Archives documentation notes that a personally identifying information in the database has been redacted for public use - which is showing up as the # characters. A memo from December 10, 1992 states the following.\nI used this to verify the correctness of my naive parsing approach. If everything lines up, the only characters we should be finding in columns 73-96, 248-271, 272-295 and 296-319 are #s.\nredacted_base_1 = [(73, 96), (248, 271), (272, 295), (296, 319)] redacted_base_0 = [(r[0] - 1, r[1]) for r in redacted_base_1] for redacted_region in redacted_base_0: start, end = redacted_region assert set(data[start:end]).pop() == \u0026quot;#\u0026quot;  Does not produce any assertion errors so lets expand to the whole dataset.\nfor i in range(0, len(data), 319): record = data[i:i+step] for redacted_region in redacted_base_0: start, end = redacted_region assert set(record[start:end]).pop() == \u0026quot;#\u0026quot;  Which also does not produce any assertion errors, so things are looking pretty good at this point. The two items of main concern are the ä characters and the large amount of whitespace. Some more investigation revealed the reason for the whitespace. The document was originally stored in a variable record length format and was converted to a fixed length by the National Archives at some point. Relevant documentation in its original photocopy glory below.\nWhile I am not as certain about the cause of the diacritic a character, I believe it is due to the use of zoned decimal formatting in some of the fields. The documentation notes its use in general, but does not provide specific indices. Since it is not affecting the integrity of the actual record parsing I am ignoring apparent zoned decimal fields for the time being.\nThe technical documentation also defines the range of each field within a record that we can use to make the final output file more explicitly delimitated. A sample of which is shown below.\nSo now it is just a matter of creating a function that will cut up a record based on the fields defined in this table. I first used PyPDF2 to pull out the text from the table I was interested in.\nimport PyPDF2 pdf = open('222.1DP.pdf', 'rb') reader = PyPDF2.PdfFileReader(pdf) pages = ''.join([reader.getPage(p).extractText() for p in [20, 21, 22, 23]]) print(pages[:100])  Which looks like this\nINPUT, OUTPUT, MASTER DEFINITION (Excluding Reports) I. PAGE 1 OF 4 5. DATE PREPARED 9/1/77 2. NAME  Its a jumble but thankfully, there are some patterns that can be exploited to reduce manual work required to get everything correctly formatted. Specifically after the range of each field it is followed by either an A or a N, signifying if that data is numeric or alphanumeric. We can use this pattern in a regex to roughly pull out what we need.\nimport re finder = re.compile(r'(.{1,7}) (\\d+-?\\d*) (N|A)') matches = finder.findall(pages) for m in matches[:10]: print(m[0], m[1])  Which prints\n. PROVe 1-2 SE'O'fO 3-8 6 ATLRG 9 !XX)RP 10 DPROV 11-12 . DDIsr 13-14 2 DVllL 15-16 . IDATX 23-26 BDA'IX 33-36 POORP 37  While this is by no means perfect it is looking a lot better. From here I just cleaned things up manually, eventually creating a text file that looks like this\nPROVC 1 2 SEQNO 3 8 ATLRG 9 DOORP 10 . . . ALTVCI 239 247 MOM 248 271 POP 272 295 ALIAS 296 319  Now we can finally create the function that will create the csv file we are after!\nFirst I created a dictionary that could be used to slice each \u0026ldquo;row\u0026rdquo; of the raw data.\nimport csv table = \u0026quot;parse_table.txt\u0026quot; spacing_dict = { r[0]: [int(i) for i in r[1:]] for r in csv.reader(open(table), delimiter=' ') } # adjust for base 1 to base 0 indexing for field in spacing_dict: spacing_dict[field][0] -= 1 # append next index for single index fields for easier slicing in next step for field_name in spacing_dict: if len(spacing_dict[field_name]) == 1: val = spacing_dict[field_name][0] spacing_dict[field_name].append(val+1)  Then I created a function that would actually do the slicing on each raw data row and return a dictionary with the field names as keys and the data in each field\u0026rsquo;s respective domain as values.\ndef raw_data_to_field_dict(raw_data, spacing_dict): field_dict = {} for field_name in spacing_dict: start, end = spacing_dict[field_name] field_dict[field_name] = raw_data[start:end] return field_dict  Now we can test this out to see if we can write our csv file.\ndef write_csv_from_raw_data(raw_data, spacing_dict, outname='data.csv'): csv_rows = [] for i in range(0, len(data), step): row = data[i:i+step] csv_rows.append(raw_data_to_field_dict(row, spacing_dict)) with open(outname, 'w') as handle: writer = csv.DictWriter(handle, fieldnames=spacing_dict.keys()) writer.writeheader() writer.writerows(csv_rows)  The function executes without incident and writes a csv file that looks like the sample below.\nPROVC\tSEQNO\tATLRG\tDOORP\tDPROV\tDDIST\tDVllL 1\t5\t7\t0\t0 1\t11\t1\t2\t0 1\t12\t1\t2\t0  Much easier to read!\nVisualizing Now that the data is in a more accessible format we can start to take examine what it looks like using R and the ggplot2 package.\nArrests by year Unsurprisingly most arrests took place between 70 and 72. Although there were some extreme early outliers in 1900 and 1903 which are likely data entry errors.\nIndividual statuses Fields 126-131 contain the OPINFO information, which the technical documentation describes as a group containing the following information, again as described by the technical documentation.\n STATUS: The status of an individual. TARGET: The type of target, general of specific. LEVEL: The level of operation. Corps, Division, etc. FORCE: Type of action force which was responsible for the neutralization. DEFAC: Place of detention.  This is a lot of interesting information. These values are all stored as one letter codes and the documentation provides tables for translating them, like the one below which is used to translate the STATUS code.\nFirst we can look at just statuses of all individuals in the database to get as sense for what was happening to the people targeted by Project Phoenix.\nWhile the STATUS field is missing for about a third of individuals in the database, clearly the most common outcomes were \u0026ldquo;captured\u0026rdquo;, \u0026ldquo;killed\u0026rdquo; or \u0026ldquo;rallied\u0026rdquo;. While \u0026ldquo;captured\u0026rdquo; and \u0026ldquo;killed\u0026rdquo; are relatively unambiguous, there is not further explanation of what \u0026ldquo;rallied\u0026rdquo; refers to that I could find.\nThis same data can also be broken down in a few interesting ways. We can plot the same STATUS data but split into into subplots based on the FORCE that was responsible for the (coldly bureaucratically termed) neutralization.\n\\\nUsing this plot, we can see that \u0026ldquo;Regional Forces\u0026rdquo; where the most active group, with \u0026ldquo;ARVN Main Forces\u0026rdquo;, \u0026ldquo;Popular Forces\u0026rdquo;, \u0026ldquo;Provincial Reconnaissance Unit\u0026rdquo; making up much of the remainder.\nThis plot also shows that US Forces, at least according to this database, where not as nearly as directly involved as organizations that can be grouped into the \u0026ldquo;South Vietnamese Allies\u0026rdquo; category.\nLastly, I was interested in what this program looked like over time. Individuals that were captured (opposed to killed outright) usually had a value in their SADATX field: \u0026ldquo;Date of sentence action in YYMM order\u0026rdquo;. I used this as a proxy for a given group\u0026rsquo;s activity over time, granted this would be easily skewed in the case one group was tasked explicitly with capturing while another was tasked with killing. Plotting SADATX vs the number of individuals for all groups listed by the FORCES field produced the plot below.\nThere is still much to be said I only looked at a small part of this dataset, but there is still much more to be gleaned. If you would like to play around with the data yourself you can download the csv file I produced from this link. You can also download all the documentation I referenced from the National Archives entry at this link.\nThank you for reading.\n-eth\n","date":"2020-12-12","permalink":"https://ethanholleman.github.io/posts/vietnam_data/","tags":["blogs","data wrangling"],"title":"Data and the Vietnam War"},{"content":"Day trip out to Putah Creek in Winters CA.\nA few turkeys we saw before leaving Davis.\nThe spot in the creek we picked out.\nErica practicing casting.\nTesting out the waders.\n","date":"2020-11-28","permalink":"https://ethanholleman.github.io/posts/putah_fishing/","tags":["fishing","blogs"],"title":"Fishing at Putah Creek"},{"content":"I ran into a few issues trying to figure out how to run batch scripts on the Genome Center cluster. One of the least documented and hardest to figure out was how to successfully load R packages I had installed to my user directory.\nFor example I would start R and install a package I needed and load the library with no problem.\nThe code below would run without a problem.\ninstall.packages(\u0026quot;glmnet\u0026quot;) library(glmnet)  But then calling the running the same script from a batch file fails to import the library.\nI tried multiple possible fixes with no success until I was tipped of by a current lab member about the magic word.\naklog  For some reason putting this right after the SBATCH lines in the script changed something that allowed SLURM to see the R packages in my user directory.\nIt is not currently documented, but when it hopefully is I will link to the page here.\n","date":"2020-11-03","permalink":"https://ethanholleman.github.io/posts/genome_cluster/","tags":["programming"],"title":"Using local R packages on UC Davis Genome Center cluster"}]